<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pytorch on ZS_</title>
    <link>https://ncepuzs.github.io/tags/pytorch/</link>
    <description>Recent content in Pytorch on ZS_</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 01 Oct 2020 17:55:28 +0800</lastBuildDate><atom:link href="https://ncepuzs.github.io/tags/pytorch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Auto_grad [Pytorch]</title>
      <link>https://ncepuzs.github.io/post/auto_grad/</link>
      <pubDate>Thu, 01 Oct 2020 17:55:28 +0800</pubDate>
      
      <guid>https://ncepuzs.github.io/post/auto_grad/</guid>
      <description>Auto_grad in Pytorch 1. 计算图 在pytorch中，如果一个变量格式为Tensor，且属性requires_grad=True，则使用这个变量产生的其他变量的requires_grad属性也将为True，同时会产生一个计算图。具体可参考 [Tensor的自动求导]: https://zhuanlan.zhihu.com/p/51385110
2. 自动求导方法： 通常情况下，只有标量scalar才能对张量tensor求导，而这个scalar通常也就是我们的loss，这也与我们的直观逻辑相符。在pytorch中常用的自动求导方法为backward()。在构造完计算图之后$J=f(x)$，便可通过$J.backward()$完成求导。
3.当tensor对张量tensor求导时 这种情况会被认为是多个loss同时求导，即$J=[loss1, loss2, loss3]$对tensor求导，其中每个loss都可以通过标量求导方法进行求导，得到的结果也是tensor的形式[grad1, grad2, grad3].
但此时不能直接使用$J.backward()$，而是要指定$J$中每个loss的权重，如果没有需要指定的权重，那就全部置为1，即$J.backward(gradients=[1.0,1.0,1.0])$即可完成tensor对tensor的求导。
4.多次求导出现错误 如果使用一个计算图进行$J$对$x$求导时，如果$x$不是我们直接定义的变量，而是一个计算图中的中间变量，那这样一次backward()就会销毁计算图，再次进行求导时就会报错。我们通常不会使用一个计算图两次，这种行为一般也是存在逻辑错误的，所以要避免这种错误。</description>
    </item>
    
  </channel>
</rss>
